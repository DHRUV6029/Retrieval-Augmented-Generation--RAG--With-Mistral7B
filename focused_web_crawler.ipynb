{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DHRUV6029/Retrieval-Augmented-Generation--RAG--With-Mistral7B/blob/main/focused_web_crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evGE8nTfDN0s",
        "outputId": "88040f34-42ad-4c28-c7ba-9bbb53bc2c9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Collecting url_normalize\n",
            "  Downloading url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from url_normalize) (1.16.0)\n",
            "Installing collected packages: url_normalize\n",
            "Successfully installed url_normalize-1.4.3\n"
          ]
        }
      ],
      "source": [
        "#!pip install url_normalize\n",
        "!pip install --upgrade nltk\n",
        "!pip install url_normalize\n",
        "\n",
        "#import necessary libraries\n",
        "import requests\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import urllib.robotparser\n",
        "import threading\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import datetime\n",
        "from url_normalize import url_normalize\n",
        "import time\n",
        "import string\n",
        "import collections\n",
        "import csv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUMX-PjiXEb1",
        "outputId": "26da8397-1023-41dd-c679-3899669a44e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cql8k0QPJ2w_",
        "outputId": "3fd15c7f-0255-4abd-e4b4-09ebbe4a34b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# global list for errors\n",
        "#defining a global error list\n",
        "errors = []\n"
      ],
      "metadata": {
        "id": "JQ8FERXDDiW3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are implementing focused crawling we will assign Priority for each Url,\n",
        "and higher the number, higher it will have the priority to crawl."
      ],
      "metadata": {
        "id": "sX0HRG48EBMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class to implement priority queue\n",
        "# the queue contains items in the format: [page_promise, url]\n",
        "class PriorityQueue:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.condition = threading.Condition()  # allows threads to wait until they are notified by another thread\n",
        "        self.queue = []\n",
        "\n",
        "    def calculate_index(self, item, start, end):\n",
        "        if len(self.queue) > 0:\n",
        "            if start < end:\n",
        "                index = int((start + end) / 2)\n",
        "                if item[0] == self.queue[index][0]:\n",
        "                    return index\n",
        "                elif item[0] > self.queue[index][0]:\n",
        "                    return self.calculate_index(item, start, index - 1)\n",
        "                elif item[0] < self.queue[index][0]:\n",
        "                    return self.calculate_index(item, index + 1, end)\n",
        "            elif start == end:\n",
        "                if end != len(self.queue):\n",
        "                    if item[0] > self.queue[start][0]:\n",
        "                        return start\n",
        "                    else:\n",
        "                        return start + 1\n",
        "                else:\n",
        "                    if item[0] < self.queue[end - 1][0]:\n",
        "                        return end\n",
        "                    else:\n",
        "                        return end - 1\n",
        "            else:\n",
        "                return start\n",
        "        else:\n",
        "            return start\n",
        "\n",
        "\n",
        "        # display the contents of the queue.\n",
        "    def display_queue(self):\n",
        "        print(\"Queue:\")\n",
        "        for item in self.queue:\n",
        "            print(item)\n",
        "\n",
        "    # add an item to the queue\n",
        "    def enqueue(self, item):\n",
        "        self.condition.acquire(True)\n",
        "\n",
        "        if item not in self.queue:\n",
        "            index = self.calculate_index(item, 0, len(self.queue))  # calculate index for new element\n",
        "            self.queue.insert(index, item)  # insert element at index\n",
        "        self.condition.notifyAll()\n",
        "        self.condition.release()\n",
        "\n",
        "    # pop an item from the queue\n",
        "    def dequeue(self):\n",
        "        self.condition.acquire(True)\n",
        "\n",
        "        while len(self.queue) <= 0:\n",
        "            self.condition.wait()\n",
        "\n",
        "        item = self.queue[0]  # item with highest promise\n",
        "        del self.queue[0]  # remove item from the queue\n",
        "        self.condition.release()\n",
        "        return item\n",
        "\n",
        "    # Returns the size of the queue\n",
        "    def get_size(self):\n",
        "        return len(self.queue)\n",
        "\n",
        "    # delete the item from the queue\n",
        "    def delete(self, index):\n",
        "        self.condition.acquire(True)\n",
        "        item = self.queue[index]\n",
        "        del self.queue[index]  # delete item at index\n",
        "        self.condition.release()\n",
        "        return item\n",
        "\n",
        "    # find a url in the queue\n",
        "    def find(self, url):\n",
        "        i = -1\n",
        "        self.condition.acquire(True)\n",
        "\n",
        "        for index in range(len(self.queue)):\n",
        "            if self.queue[index][1] == url:\n",
        "                i = index\n",
        "        self.condition.release()\n",
        "        return i\n",
        "\n",
        "    # update the promise of a url if it is found while parsing another page\n",
        "    def update_queue(self, url, parent_relevance):\n",
        "        self.condition.acquire()\n",
        "\n",
        "        index = self.find(url)\n",
        "        if index != -1:\n",
        "            item = self.queue[index]\n",
        "            del self.queue[index]  # remove item from queue\n",
        "            item[0] += 0.25 * parent_relevance\n",
        "\n",
        "            self.enqueue(item)\n",
        "\n",
        "        self.condition.notifyAll()\n",
        "        self.condition.release()\n",
        "\n"
      ],
      "metadata": {
        "id": "ohMRcmccD7yV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # class to implement the parsed_urls dictionary\n",
        "# the dictionary has visited urls as keys and values as a list of [links_found, promise, len, time]\n",
        "# links_found are the links found while parsing the page\n",
        "# promise is the page relevance promise\n",
        "# len is the page length\n",
        "# time is the time at which the page was parsed\n",
        "class ParsedURLs:\n",
        "    def __init__(self):\n",
        "        self.lock = threading.Lock()\n",
        "        self.parsed_urls = collections.OrderedDict()  # to remember the order in which URLs (keys) were added\n",
        "\n",
        "    def add_item(self, url, links_found, promise, relevance, len, status_code, time):  # add an item into the dictionary\n",
        "        self.lock.acquire()\n",
        "        self.parsed_urls[url] = [links_found, promise, relevance, len, status_code, time]\n",
        "        self.lock.release()\n",
        "\n",
        "    def find(self, url):  # check if item already exists\n",
        "        return url in self.parsed_urls\n",
        "\n",
        "    def display(self):  # display URLs in dictionary i.e. the keys\n",
        "        print(self.parsed_urls.keys())\n",
        "\n",
        "    def get_keys(self):  # return all the keys of the dictionary\n",
        "        return self.parsed_urls.keys()\n",
        "\n",
        "    def get_item(self, key):  # return the number of links found, promise, page len, timestamp for a given key\n",
        "        return len(self.parsed_urls[key][0]), self.parsed_urls[key][1], self.parsed_urls[key][2], \\\n",
        "               self.parsed_urls[key][3], self.parsed_urls[key][4], self.parsed_urls[key][5]"
      ],
      "metadata": {
        "id": "WaB0SDYXE155"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class to keep track of page count i.e. number of pages crawled\n",
        "class PageCount:\n",
        "    def __init__(self):\n",
        "        self.lock = threading.Lock()\n",
        "        self.page_num = 0\n",
        "\n",
        "    def increment(self):\n",
        "        self.lock.acquire()\n",
        "        self.page_num += 1\n",
        "        self.lock.release()\n",
        "\n",
        "    def get_page_num(self):\n",
        "        return self.page_num\n",
        "\n",
        "\n",
        "page_count = PageCount()\n"
      ],
      "metadata": {
        "id": "TJNTZCLuE7gf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class to perform multi-threaded crawling\n",
        "class CrawlerThread(threading.Thread):\n",
        "    def __init__(self, links_to_parse, parsed_urls, query, pages, page_link_limit, mode):\n",
        "        # initializing all the thread attributes\n",
        "        threading.Thread.__init__(self)\n",
        "        self.links_to_parse = links_to_parse\n",
        "        self.parsed_urls = parsed_urls\n",
        "        self.query = query\n",
        "        self.stoprequest = threading.Event()  # to stop a thread\n",
        "        self.pages = pages\n",
        "        self.page_link_limit = page_link_limit\n",
        "        self.mode = mode\n",
        "\n",
        "    def join(self, timeout=None):  # waits for the thread to finish executing or until timeout occurs\n",
        "        super(CrawlerThread, self).join(timeout)\n",
        "\n",
        "    def run(self):\n",
        "        item = self.links_to_parse.dequeue()  # get first item (highest promise) from the queue, item = [promise,url]\n",
        "        print('Dequeued: ', item)\n",
        "        url = item[1]\n",
        "        html_text, links = visit_url(url, self.page_link_limit)  # read the HTML content of the URL, extract links\n",
        "        while (html_text, links) == (None, None):  # keep trying till visit_url() returns non-None values\n",
        "            item = self.links_to_parse.dequeue()\n",
        "            url = item[1]\n",
        "            html_text, links = visit_url(url)\n",
        "\n",
        "        page_count.increment()  # increment the page counter\n",
        "        print(page_count.get_page_num())\n",
        "\n",
        "        relevance = get_relevance(html_text, self.query)  # get relevance of a URL after visiting it\n",
        "        # will use it to compute promise of its child links\n",
        "\n",
        "        self.parsed_urls.add_item(url, links, item[0], relevance, len(html_text), requests.get(url).status_code, str(\n",
        "            datetime.datetime.now().time()))  # add the crawled URL and details into the dictionary parsed_urls\n",
        "        print('Parsed: ', item)\n",
        "\n",
        "        for index in range(len(links)):  # add all the links present in the page to the queue\n",
        "            if links[index] in self.parsed_urls.get_keys():\n",
        "                continue\n",
        "            else:\n",
        "                id = self.links_to_parse.find(links[index])  # check if the URL is already present in the queue\n",
        "                if id != -1:\n",
        "                    # URL already present in the queue\n",
        "                    if self.mode == 'bfs':\n",
        "                        pass\n",
        "                    else:  # for focused crawling, update the promise of the link using parent's relevance\n",
        "                        self.links_to_parse.update_queue(links[index], relevance)  # update item, pass parent relevance\n",
        "                else:\n",
        "                    # URL not in the queue\n",
        "                    if validate_link(links[index]):\n",
        "                        promise = get_promise(self.query, links[index], self.mode, relevance)  # relevance is of parent\n",
        "                        new_item = [promise, links[index]]\n",
        "                        self.links_to_parse.enqueue(new_item)"
      ],
      "metadata": {
        "id": "jOrjo7i7E-td"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we will get start pages by performing google search\n",
        "def get_start_pages(query, num_start_pages=10):\n",
        "\n",
        "\n",
        "    res = requests.get('https://www.google.com/search', params={'q': query})\n",
        "    soup = BeautifulSoup(res.content, 'lxml')\n",
        "    links = soup.find_all('a')\n",
        "\n",
        "    initial_links = []\n",
        "    count = 0\n",
        "\n",
        "    for link in links:\n",
        "        href = link.get('href')\n",
        "        if \"url?q=\" in href and \"webcache\" not in href:\n",
        "            l_new = href.split(\"?q=\")[1].split(\"&sa=U\")[0]\n",
        "            if validate_link(url_normalize(l_new)):  # checking if normalized URL is crawlable\n",
        "                count += 1\n",
        "                if count <= num_start_pages:\n",
        "                    initial_links.append(url_normalize(l_new))\n",
        "                else:\n",
        "                    break\n",
        "    return list(set(initial_links))"
      ],
      "metadata": {
        "id": "1CA_RiF6FLbg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_link(url):\n",
        "    \"\"\" checks if website is crawlable (status code 200) and if its robots.txt allows crawling\n",
        "    also checks for file types in the url and for a list of words to be excluded \"\"\"\n",
        "\n",
        "    excluded_words = ['download', 'upload', 'javascript', 'cgi', 'file']\n",
        "    excluded_types = [\".asx\", \".avi\", \".bmp\", \".css\", \".doc\", \".docx\",\n",
        "                      \".flv\", \".gif\", \".jpeg\", \".jpg\", \".mid\", \".mov\",\n",
        "                      \".mp3\", \".ogg\", \".pdf\", \".png\", \".ppt\", \".ra\",\n",
        "                      \".ram\", \".rm\", \".swf\", \".txt \", \".wav\", \".wma\",\n",
        "                      \".wmv\", \".xml\", \".zip\", \".m4a\", \".m4v\", \".mov\",\n",
        "                      \".mp4\", \".m4b\", \".cgi\", \".svg\", \".ogv\", \".dmg\", \".tar\", \".gz\"]\n",
        "\n",
        "    for ex_word in excluded_words:\n",
        "        if ex_word in url.lower():\n",
        "            errors.append('Link contains excluded terms')\n",
        "            return False\n",
        "\n",
        "    for ex_type in excluded_types:\n",
        "        if ex_type in url.lower():\n",
        "            errors.append('Link contains excluded type')\n",
        "            return False\n",
        "\n",
        "    # checking if the url returns a status code 200\n",
        "    try:\n",
        "        r = requests.get(url)\n",
        "        if r.status_code == 200:\n",
        "            pass  # website returns status code 200, so check for robots.txt\n",
        "        else:\n",
        "            print(url, r.status_code, 'failed')\n",
        "            errors.append(r.status_code)\n",
        "            return False\n",
        "    except:\n",
        "        print(url, 'request failed')  # request failed\n",
        "        errors.append('Request Failed')\n",
        "        return False\n",
        "\n",
        "    # checking if the website has a robots.txt, and then checking if I am allowed to crawl it\n",
        "    domain = urlparse(url).scheme + '://' + urlparse(url).netloc\n",
        "\n",
        "    try:\n",
        "        rp = urllib.robotparser.RobotFileParser()\n",
        "        rp.set_url(domain + '/robots.txt')\n",
        "        rp.read()\n",
        "        if not rp.can_fetch('*', url):  # robots.txt mentions that the link should not be parsed\n",
        "            print('robots.txt does not allow to crawl', url)\n",
        "            errors.append('Robots Exclusion')\n",
        "            return False\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "CiVHqPKOFTtb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input():\n",
        "    \"\"\" get query, number of start pages, number of pages to be returned and mode \"\"\"\n",
        "\n",
        "    query = input('Enter your query (default: \"wildfires california\"): ').strip()\n",
        "    num_start_pages = input(\"Enter the number of start pages (default: 10): \").strip()\n",
        "    n = input(\"Enter the number of pages to be returned (at least 10, default: 1000): \").strip()\n",
        "    page_link_limit = input(\"Enter the max. no. of links to be fetched from each page (at least 10, default: 30): \")\\\n",
        "        .strip()\n",
        "    mode = input(\"Enter mode 'bfs' or 'focused' (default: 'bfs'): \").strip()\n",
        "\n",
        "    print('\\nObtaining start pages...\\n')\n",
        "    # checking if values are input correctly, otherwise use defaults\n",
        "    if len(query) == 0:\n",
        "        query = 'wildfires california'\n",
        "\n",
        "    if len(num_start_pages) == 0 or int(num_start_pages) <= 0:\n",
        "        num_start_pages = 10\n",
        "\n",
        "    if len(n) == 0 or int(n) < 10:\n",
        "        n = 1000\n",
        "\n",
        "    if len(page_link_limit) == 0 or int(page_link_limit) < 10:\n",
        "        page_link_limit = 30\n",
        "\n",
        "    if len(mode) == 0 or mode.lower() not in {'bfs', 'focused'}:\n",
        "        mode = 'bfs'\n",
        "\n",
        "    return query, int(num_start_pages), int(n), int(page_link_limit), mode\n"
      ],
      "metadata": {
        "id": "hCkqP2JRFam1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculates the priority of each URL\n",
        "\n",
        "def get_promise(query, url, mode, parent_relevance):\n",
        "    \"\"\" returns the promise of a URL, based on which URLs are placed on the priority queue \"\"\"\n",
        "    if mode.lower() == 'bfs':\n",
        "        return 1  # all pages have the same promise in a simple bfs crawl since we do not compute relevance\n",
        "    else:\n",
        "        # calculate promise based on the link\n",
        "        promise = 0\n",
        "\n",
        "        # remove punctuation from query\n",
        "        punctuation = set(string.punctuation)\n",
        "        query = ''.join(x for x in query if x not in punctuation)\n",
        "\n",
        "        query_terms = [q.lower() for q in query.strip().split()]\n",
        "\n",
        "        # checking if all or any of the terms are in the link, if synonyms are present, if lemmatized words are present\n",
        "        synonyms, lemmatized_words = get_synonyms_and_lemmatized(query.lower())\n",
        "\n",
        "        # synonyms is a dict with a list of synonyms per query term\n",
        "        # creating a combined list of synonyms, without duplicates\n",
        "        synonyms_list = list(set([s for sublist in list(synonyms.values()) for s in sublist]))\n",
        "\n",
        "        if all([x in url.lower() for x in query_terms]):  # all query terms are in the URL\n",
        "            promise += 0.5\n",
        "        elif any([x in url.lower() for x in query_terms]):  # at least 1 query term in URL, but not all\n",
        "            promise += 0.25\n",
        "        else:  # no query term in URL\n",
        "            pass  # keep promise as is\n",
        "\n",
        "        # checking for synonyms\n",
        "        if all([x in url.lower() for x in synonyms_list]):  # all synonyms are in the URL\n",
        "            promise += 0.4\n",
        "        elif any([x in url.lower() for x in synonyms_list]):  # at least 1 synonym is in URL, but not all\n",
        "            promise += 0.2\n",
        "        else:  # no synonym in URL\n",
        "            pass  # keep promise as is\n",
        "\n",
        "        # checking for lemmatized words\n",
        "        if all([x in url.lower() for x in lemmatized_words]):  # all lemmatized words are in the URL\n",
        "            promise += 0.4\n",
        "        elif any([x in url.lower() for x in lemmatized_words]):  # at least 1 lemmatized word is in URL, but not all\n",
        "            promise += 0.2\n",
        "        else:  # no lemmatized word in URL\n",
        "            pass  # keep promise as is\n",
        "\n",
        "        promise += 0.25 * parent_relevance  # giving a certain weight to URL's parent's relevance\n",
        "        promise /= len(url)  # to penalize longer URLs\n",
        "        return promise\n"
      ],
      "metadata": {
        "id": "I3Qao1HLFduF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevance(html_text, query):\n",
        "    \"\"\" returns the relevance of a page after crawling it \"\"\"\n",
        "\n",
        "    # remove punctuation from query\n",
        "    punctuation = set(string.punctuation)\n",
        "    query = ''.join(x for x in query if x not in punctuation)\n",
        "\n",
        "    query_terms = query.lower().strip().split()\n",
        "    relevance = 0\n",
        "\n",
        "    synonyms, lemmatized_words = get_synonyms_and_lemmatized(query)\n",
        "    synonyms_list = list(set([s for sublist in list(synonyms.values()) for s in sublist]))\n",
        "\n",
        "    soup = BeautifulSoup(html_text, 'lxml')\n",
        "\n",
        "    if soup.title:\n",
        "        # TITLE\n",
        "        title = soup.title.text.lower()\n",
        "        # checking query terms -----------------------------------------\n",
        "        if all([q in title for q in query_terms]):  # all terms in title\n",
        "            relevance += 0.25\n",
        "        elif any([q in title for q in query_terms]):  # at least one term in title but not all\n",
        "            relevance += 0.15\n",
        "        else:\n",
        "            pass  # keep relevance as is\n",
        "\n",
        "        # checking synonyms_list terms ----------------------------------\n",
        "        if all([q in title for q in synonyms_list]):  # all terms in title\n",
        "            relevance += 0.2\n",
        "        elif any([q in title for q in synonyms_list]):  # at least one term in title but not all\n",
        "            relevance += 0.1\n",
        "        else:\n",
        "            pass  # keep relevance as is\n",
        "\n",
        "        # checking lemmatized words -----------------------------------------\n",
        "        if all([q in title for q in lemmatized_words]):  # all terms in title\n",
        "            relevance += 0.2\n",
        "        elif any([q in title for q in lemmatized_words]):  # at least one term in title but not all\n",
        "            relevance += 0.1\n",
        "        else:\n",
        "            pass  # keep relevance as is\n",
        "\n",
        "    if soup.find('h1'):\n",
        "        # FIRST HEADING\n",
        "        h1 = soup.find('h1').text.lower()  # first h1 heading\n",
        "\n",
        "        # checking query terms -----------------------------------------\n",
        "        if all([q in h1 for q in query_terms]):  # all terms in first heading\n",
        "            relevance += 0.5\n",
        "        elif any([q in h1 for q in query_terms]):  # at least one term in heading but not all\n",
        "            relevance += 0.25\n",
        "        else:\n",
        "            pass  # keep relevance as is\n",
        "\n",
        "        # checking synonyms_list terms ----------------------------------\n",
        "        if all([q in h1 for q in synonyms_list]):  # all terms in first heading\n",
        "            relevance += 0.45\n",
        "        elif any([q in h1 for q in synonyms_list]):  # at least one term in heading but not all\n",
        "            relevance += 0.2\n",
        "        else:\n",
        "            pass  # keep relevance as is\n",
        "\n",
        "        # checking lemmatized words -----------------------------------------\n",
        "        if all([q in h1 for q in lemmatized_words]):  # all terms in first heading\n",
        "            relevance += 0.45\n",
        "        elif any([q in h1 for q in lemmatized_words]):  # at least one term in heading but not all\n",
        "            relevance += 0.2\n",
        "        else:\n",
        "            pass  # keep relevance as is\n",
        "\n",
        "    if soup.find_all('a'):\n",
        "        # ANCHOR TAGS TEXT\n",
        "        a_text = ' '.join(list(set([a.text.lower() for a in soup.find_all('a')])))  # anchor tags text combined\n",
        "\n",
        "        # checking query terms -----------------------------------------\n",
        "        if all([q in a_text for q in query_terms]):  # all terms in anchor text\n",
        "            relevance += 0.25\n",
        "        elif any([q in a_text for q in query_terms]):  # at least one term in anchor text but not all\n",
        "            relevance += 0.15\n",
        "        else:\n",
        "            pass  # keep relevance as is\n",
        "\n",
        "        # checking synonyms_list terms ----------------------------------\n",
        "        if all([q in a_text for q in synonyms_list]):  # all terms in anchor text\n",
        "            relevance += 0.2\n",
        "        elif any([q in a_text for q in synonyms_list]):  # at least one term in anchor text but not all\n",
        "            relevance += 0.1\n",
        "        else:\n",
        "            pass  # keep relevance as is\n",
        "\n",
        "        # checking lemmatized words -----------------------------------------\n",
        "        if all([q in a_text for q in lemmatized_words]):  # all terms in anchor text\n",
        "            relevance += 0.2\n",
        "        elif any([q in a_text for q in lemmatized_words]):  # at least one term in anchor text but not all\n",
        "            relevance += 0.1\n",
        "        else:\n",
        "            pass  # keep relevance as is\n",
        "\n",
        "    if soup.find_all('b'):\n",
        "        # BOLD TEXT\n",
        "        bold = ' '.join(list(set([b.text.lower() for b in soup.find_all('b')])))  # bold text combined\n",
        "\n",
        "        # checking query terms -----------------------------------------\n",
        "        if all([q in bold for q in query_terms]):  # all terms in bold text\n",
        "            relevance += 0.25\n",
        "        elif any([q in bold for q in query_terms]):  # at least one term in bold text but not all\n",
        "            relevance += 0.15\n",
        "        else:\n",
        "            pass  # keep relevance as is\n",
        "\n",
        "        # checking synonyms_list terms ----------------------------------\n",
        "        if all([q in bold for q in synonyms_list]):  # all terms in bold text\n",
        "            relevance += 0.2\n",
        "        elif any([q in bold for q in synonyms_list]):  # at least one term in bold text but not all\n",
        "            relevance += 0.1\n",
        "        else:\n",
        "            pass  # keep relevance as is\n",
        "\n",
        "        # checking lemmatized words -----------------------------------------\n",
        "        if all([q in bold for q in lemmatized_words]):  # all terms in bold text\n",
        "            relevance += 0.2\n",
        "        elif any([q in bold for q in lemmatized_words]):  # at least one term in bold text but not all\n",
        "            relevance += 0.1\n",
        "        else:\n",
        "            pass  # keep relevance as is\n",
        "\n",
        "    # REMAINING PAGE TEXT\n",
        "    remove_checked = [s.extract() for s in soup(['title', 'b', 'a', 'h1'])]  # remove title, anchors, h1 and bold text\n",
        "    page_text = soup.text.replace('\\n', '').lower()  # page text (after extracting already checked tags)\n",
        "\n",
        "    # checking query terms -----------------------------------------\n",
        "    if all([q in page_text for q in query_terms]):  # all terms in remaining text\n",
        "        relevance += 0.5\n",
        "    elif any([q in page_text for q in query_terms]):  # at least one term in remaining text but not all\n",
        "        relevance += 0.25\n",
        "    else:\n",
        "        pass  # keep relevance as is\n",
        "\n",
        "    # checking synonyms_list terms ----------------------------------\n",
        "    if all([q in page_text for q in synonyms_list]):  # all terms in remaining text\n",
        "        relevance += 0.45\n",
        "    elif any([q in page_text for q in synonyms_list]):  # at least one term in remaining text but not all\n",
        "        relevance += 0.2\n",
        "    else:\n",
        "        pass  # keep relevance as is\n",
        "\n",
        "    # checking lemmatized words -----------------------------------------\n",
        "    if all([q in page_text for q in lemmatized_words]):  # all terms in remaining text\n",
        "        relevance += 0.45\n",
        "    elif any([q in page_text for q in lemmatized_words]):  # at least one term in remaining text but not all\n",
        "        relevance += 0.2\n",
        "    else:\n",
        "        pass  # keep relevance as is\n",
        "\n",
        "    return relevance"
      ],
      "metadata": {
        "id": "zmo4oIMvFpqY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synonyms_and_lemmatized(query):\n",
        "    \"\"\" returns a dict with a list of synonyms per word in the query \"\"\"\n",
        "    words = word_tokenize(query)\n",
        "    pos = {}\n",
        "    for word in words:\n",
        "        pos.update({word: pos_tag([word], tagset='universal')[0][1]})\n",
        "\n",
        "    simplified_pos_tags = {}\n",
        "\n",
        "    for x in pos.keys():\n",
        "        if pos[x] == 'NOUN':\n",
        "            simplified_pos_tags.update({x: 'n'})\n",
        "        elif pos[x] == 'VERB':\n",
        "            simplified_pos_tags.update({x: 'v'})\n",
        "        elif pos[x] == 'ADJ':\n",
        "            simplified_pos_tags.update({x: 'a'})\n",
        "        elif pos[x] == 'ADV':\n",
        "            simplified_pos_tags.update({x: 'r'})\n",
        "        else:\n",
        "            simplified_pos_tags.update({x: 'n'})  # consider everything else to be a noun\n",
        "\n",
        "    synonyms = {}\n",
        "    for w in words:\n",
        "        synonyms[w] = []\n",
        "\n",
        "    for w in words:\n",
        "        if len(wordnet.synsets(w, pos=simplified_pos_tags[w])) != 0:\n",
        "            s = [x.lower().replace('_', ' ') for x in wordnet.synsets(w, pos=simplified_pos_tags[w])[0].lemma_names() if\n",
        "                 x.lower() != w]\n",
        "            for x in s:\n",
        "                if x not in synonyms[w]:\n",
        "                    synonyms[w].append(x)\n",
        "\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    # lemmatize all words, return only those which aren't the same as the word\n",
        "    lemmatized_words = [wordnet_lemmatizer.lemmatize(w, simplified_pos_tags[w]) for w in words if\n",
        "                        wordnet_lemmatizer.lemmatize(w, simplified_pos_tags[w]) != w]\n",
        "\n",
        "    return synonyms, list(set(lemmatized_words))\n"
      ],
      "metadata": {
        "id": "nx0kX4GQFxW1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visit_url(url, page_link_limit):\n",
        "    \"\"\" parses a page to extract text and first k links; returns HTML text and normalized links \"\"\"\n",
        "\n",
        "    try:\n",
        "        res = requests.get(url)\n",
        "        if res.status_code == 200 and 'text/html' in res.headers['Content-Type']:  # also checking MIME type\n",
        "            html_text = res.text\n",
        "            soup = BeautifulSoup(res.content, 'lxml')\n",
        "            f_links = soup.find_all('frame')\n",
        "            a_links = soup.find_all('a')\n",
        "\n",
        "            # check if the page has a <base> tag to get the base URL for relative links\n",
        "            base = soup.find('base')\n",
        "            if base is not None:\n",
        "                base_url = base.get('href')\n",
        "            else:\n",
        "                # construct the base URL\n",
        "                scheme = urlparse(url).scheme\n",
        "                domain = urlparse(url).netloc\n",
        "                base_url = scheme + '://' + domain\n",
        "\n",
        "            src = [urljoin(base_url, f.get('src')) for f in f_links]\n",
        "            href = [urljoin(base_url, a.get('href')) for a in a_links]\n",
        "\n",
        "            links = list(set(src + href))[:page_link_limit]\n",
        "            links = [url_normalize(l) for l in links if validate_link(url_normalize(l))]\n",
        "\n",
        "            return html_text, links\n",
        "        else:\n",
        "            return None, None\n",
        "    except:\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "BIxLhmtDF1n0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_harvest_rate(parsed_urls, threshold):\n",
        "    \"\"\" return harvest rate i.e. # relevant links/# total links parsed \"\"\"\n",
        "\n",
        "    total_parsed = len(parsed_urls.get_keys())\n",
        "    total_relevant = 0\n",
        "\n",
        "    for link in parsed_urls.get_keys():\n",
        "        if parsed_urls.get_item(link)[2] >= threshold:\n",
        "            total_relevant += 1\n",
        "\n",
        "    harvest_rate = total_relevant/total_parsed\n",
        "\n",
        "    return harvest_rate"
      ],
      "metadata": {
        "id": "azgg21DIF4Vq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_log(parsed_urls, query, num_start_pages, num_crawled, page_link_limit, n, mode, harvest_rate, threshold,\n",
        "               total_time):\n",
        "    \"\"\" creates a log file for the crawler \"\"\"\n",
        "    file = open('crawler_log.txt', 'w')\n",
        "\n",
        "    total_size = 0\n",
        "\n",
        "    file.write('Query: ' + query + '\\n')\n",
        "    file.write('Number of Crawlable Start Pages: ' + str(num_start_pages) + '\\n')\n",
        "    file.write('Number of URLs to be Crawled: ' + str(n) + '\\n')\n",
        "    file.write('Max. Number of Links to be Scraped per Page: ' + str(page_link_limit) + '\\n')\n",
        "    file.write('Crawl Mode: ' + mode + '\\n')\n",
        "    file.write('Number of URLs Crawled: ' + str(num_crawled) + '\\n\\n')\n",
        "\n",
        "    file.write('URLs Crawled:\\n')\n",
        "    file.write('-------------\\n\\n')\n",
        "\n",
        "    counter = 0\n",
        "    for p in parsed_urls.get_keys():\n",
        "        file.write(str(counter+1) + '. \\n')\n",
        "        file.write('URL:' + p + '\\n')\n",
        "        num_links, page_promise, relevance, page_size, status_code, timestamp = parsed_urls.get_item(p)\n",
        "        total_size += page_size\n",
        "\n",
        "        file.write('Number of Links in Page:' + str(num_links) + '\\n')\n",
        "        file.write('Page Size:' + str(page_size) + '\\n')\n",
        "        file.write('Page Promise: ' + str(page_promise) + '\\n')\n",
        "        file.write('Page Relevance: ' + str(relevance) + '\\n')\n",
        "        file.write('Status Code: ' + str(status_code) + '\\n')\n",
        "        file.write('Crawled at:' + str(timestamp) + '\\n')\n",
        "        file.write('\\n\\n')\n",
        "        counter += 1\n",
        "    file.write('Total Size of all Pages Crawled: ' + str(total_size) + '\\n')\n",
        "    if total_time < 1:  # convert to seconds\n",
        "        total_time *= 60\n",
        "        file.write('Total Time Elapsed: ' + str(total_time) + ' sec\\n')\n",
        "    else:\n",
        "        file.write('Total Time Elapsed: ' + str(total_time) + ' min\\n')\n",
        "\n",
        "    file.write('Harvest Rate: ' + str(harvest_rate) + ' at Threshold: ' + str(threshold) + '\\n')\n",
        "\n",
        "    unique_errors = list(set(errors))\n",
        "    file.write('Errors: \\n')\n",
        "    for e in unique_errors:\n",
        "        file.write(str(e) + ': ' + str(errors.count(e)) + '\\n')"
      ],
      "metadata": {
        "id": "TbxwbVTqF7Er"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    query, num_start_pages, n, page_link_limit, mode = get_input()\n",
        "    start_time = time.time()\n",
        "    start_pages = get_start_pages(query, num_start_pages)\n",
        "\n",
        "    links_to_parse = PriorityQueue()\n",
        "    parsed_urls = ParsedURLs()\n",
        "\n",
        "    print('Found %d crawlable start pages:\\n' % len(start_pages))\n",
        "    # enqueue the start pages after computing their promises\n",
        "    for s in start_pages:\n",
        "        promise = get_promise(query, s, mode, 0)  # initially, parent_relevance is 0\n",
        "        links_to_parse.enqueue([promise, s])\n",
        "\n",
        "    # display the queue\n",
        "    links_to_parse.display_queue()\n",
        "    print('\\n')\n",
        "\n",
        "    threads = []\n",
        "\n",
        "    for i in range(n):  # creating threads for faster crawling\n",
        "        threads.append(CrawlerThread(links_to_parse, parsed_urls, query, n, page_link_limit, mode))\n",
        "        threads[i].daemon = True  # daemon threads allow the program to quit even if they are still running\n",
        "        threads[i].start()\n",
        "\n",
        "    while True:\n",
        "        if page_count.get_page_num() == n or links_to_parse == []:  # crawled n pages (or) priority queue empty\n",
        "            end_time = time.time()\n",
        "            total_time = (end_time - start_time)/60  # minutes\n",
        "\n",
        "            # compute harvest rate\n",
        "            threshold = 3  # relevance threshold\n",
        "            harvest_rate = get_harvest_rate(parsed_urls, threshold)\n",
        "\n",
        "            # create a crawler log file\n",
        "            create_log(parsed_urls, query, len(start_pages), len(parsed_urls.get_keys()), page_link_limit, n, mode,\n",
        "                       harvest_rate, threshold, total_time)\n",
        "            break"
      ],
      "metadata": {
        "id": "BhdEZU_KG7_F"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cshm_D9AHQKt",
        "outputId": "e1fd6e4b-369e-482f-f136-d08fe3c4856e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query (default: \"wildfires california\"): us stock market\n",
            "Enter the number of start pages (default: 10): 10\n",
            "Enter the number of pages to be returned (at least 10, default: 1000): 10\n",
            "Enter the max. no. of links to be fetched from each page (at least 10, default: 30): 20\n",
            "Enter mode 'bfs' or 'focused' (default: 'bfs'): focused\n",
            "\n",
            "Obtaining start pages...\n",
            "\n",
            "https://www.barrons.com/livecoverage/stock-market-today-112823 403 failed\n",
            "https://www.visualcapitalist.com/largest-stock-exchanges-in-the-world/ 403 failed\n",
            "https://money.usnews.com/investing/articles/will-the-stock-market-crash-risk-factors request failed\n",
            "https://www.cnbc.com/us-market-movers/ request failed\n",
            "Found 10 crawlable start pages:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-351080ccf27d>:49: DeprecationWarning: notifyAll() is deprecated, use notify_all() instead\n",
            "  self.condition.notifyAll()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queue:\n",
            "[0.0203125, 'https://www.cnbc.com/us-markets/']\n",
            "[0.009701492537313432, 'https://www.cnbc.com/2023/11/30/stock-market-today-live-update.html']\n",
            "[0.00743801652892562, 'https://www.reuters.com/markets/us/wall-st-week-ahead-tax-loss-selling-santa-rally-could-sway-us-stocks-after-2023-12-02/']\n",
            "[0.007065217391304348, 'https://www.reuters.com/markets/us/futures-rise-growing-bets-rate-cuts-next-year-2023-11-29/']\n",
            "[0.006842105263157895, 'https://news.yahoo.com/north-korea-military-us-spy-satellite-how-did-we-get-here-120037976.html']\n",
            "[0.005803571428571429, 'https://www.bloomberg.com/news/articles/2023-11-26/asian-stocks-to-rise-as-fear-gauge-hits-2020-low-markets-wrap']\n",
            "[0.00510204081632653, 'https://www.cnbc.com/2023/12/01/asia-markets.html']\n",
            "[0.002688172043010753, 'https://www.cnn.com/2023/12/01/investing/premarket-stocks-trading-2023-santa-rally/index.html']\n",
            "[0.0, 'https://www.cnbc.com/dow-30/']\n",
            "[0.0, 'https://www.cnbc.com/nasdaq-100/']\n",
            "\n",
            "\n",
            "Dequeued:  [0.0203125, 'https://www.cnbc.com/us-markets/']\n",
            "Dequeued:  [0.009701492537313432, 'https://www.cnbc.com/2023/11/30/stock-market-today-live-update.html']\n",
            "Dequeued:  [0.00743801652892562, 'https://www.reuters.com/markets/us/wall-st-week-ahead-tax-loss-selling-santa-rally-could-sway-us-stocks-after-2023-12-02/']Dequeued:  [0.007065217391304348, 'https://www.reuters.com/markets/us/futures-rise-growing-bets-rate-cuts-next-year-2023-11-29/']\n",
            "\n",
            "Dequeued:  [0.006842105263157895, 'https://news.yahoo.com/north-korea-military-us-spy-satellite-how-did-we-get-here-120037976.html']\n",
            "Dequeued:  [0.005803571428571429, 'https://www.bloomberg.com/news/articles/2023-11-26/asian-stocks-to-rise-as-fear-gauge-hits-2020-low-markets-wrap']\n",
            "Dequeued:  [0.00510204081632653, 'https://www.cnbc.com/2023/12/01/asia-markets.html']\n",
            "Dequeued:  [0.002688172043010753, 'https://www.cnn.com/2023/12/01/investing/premarket-stocks-trading-2023-santa-rally/index.html']\n",
            "Dequeued:  [0.0, 'https://www.cnbc.com/dow-30/']\n",
            "Dequeued:  [0.0, 'https://www.cnbc.com/nasdaq-100/']\n",
            "1\n",
            "Parsed:  [0.005803571428571429, 'https://www.bloomberg.com/news/articles/2023-11-26/asian-stocks-to-rise-as-fear-gauge-hits-2020-low-markets-wrap']\n",
            "https://www.reuters.com/markets/companies/AAPL.O 401 failed\n",
            "https://www.refinitiv.com/en/products/refinitiv-workspace/?utm_campaign=Reuters_ProductPage_Links&utm_medium=footer 404 failed\n",
            "https://www.reutersagency.com/en/?utm_campaign=site-referral&utm_content=us&utm_medium=reuters&utm_term=0 403 failed\n",
            "https://www.linkedin.com/company/10256858/ 999 failed\n",
            "https://www.refinitiv.com/en/products/refinitiv-workspace/?utm_campaign=Reuters_ProductPage_Links&utm_medium=articlebanner 404 failed\n",
            "https://www.reuters.com/markets/companies/NTAP.O 401 failed\n",
            "https://www.linkedin.com/company/10256858/ 999 failed\n",
            "robots.txt does not allow to crawl https://www.instagram.com/Reuters\n",
            "https://www.reutersagency.com/en/about/about-us/ 403 failed\n",
            "2\n",
            "Parsed:  [0.00743801652892562, 'https://www.reuters.com/markets/us/wall-st-week-ahead-tax-loss-selling-santa-rally-could-sway-us-stocks-after-2023-12-02/']\n",
            "https://www.reuters.com/markets/quote/.SPX 401 failed\n",
            "3\n",
            "robots.txt does not allow to crawl https://facebook.com/cnnbusiness\n",
            "4\n",
            "https://www.wsj.com/business/media/apple-and-paramount-discuss-bundling-their-streaming-services-226972d1 403 failed\n",
            "Parsed:  [0.007065217391304348, 'https://www.reuters.com/markets/us/futures-rise-growing-bets-rate-cuts-next-year-2023-11-29/']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-351080ccf27d>:99: DeprecationWarning: notifyAll() is deprecated, use notify_all() instead\n",
            "  self.condition.notifyAll()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "Parsed:  [0.00510204081632653, 'https://www.cnbc.com/2023/12/01/asia-markets.html']\n",
            "Parsed:  [0.0, 'https://www.cnbc.com/dow-30/']\n",
            "Parsed:  [0.0203125, 'https://www.cnbc.com/us-markets/']\n",
            "Parsed:  [0.0, 'https://www.cnbc.com/nasdaq-100/']\n",
            "robots.txt does not allow to crawl https://instagram.com/cnnbusiness\n",
            "9\n",
            "Parsed:  [0.009701492537313432, 'https://www.cnbc.com/2023/11/30/stock-market-today-live-update.html']\n",
            "10\n"
          ]
        }
      ]
    }
  ]
}